<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Dealing with Failed Nodes &mdash; Project Clearwater 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="Project Clearwater 1.0 documentation" href="index.html" />
    <link rel="next" title="C++ Coding Guidelines" href="Clearwater_CPP_Coding_Guidelines.html" />
    <link rel="prev" title="Provisioning Subscribers" href="Provisioning_Subscribers.html" />
   
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="dealing-with-failed-nodes">
<h1>Dealing with Failed Nodes<a class="headerlink" href="#dealing-with-failed-nodes" title="Permalink to this headline">¶</a></h1>
<p>Nodes can be easily removed from a Clearwater deployment by following
the instructions for <a class="reference external" href="Clearwater_Elastic_Scaling.md">elastic
scaling</a>. When scaling down the
remaining nodes are informed that a node is leaving and take appropriate
action. However sometimes a node may fail unexpectedly. If this happens
and the node cannot be recovered (for example the virtual machine has
been deleted) the remaining nodes must be manually informed of the
failure. This article explains how to do this.</p>
<p>The processes described in the document do not affect call processing
and can be run on a system handling call traffic.</p>
<div class="section" id="removing-a-failed-node">
<h2>Removing a Failed Node<a class="headerlink" href="#removing-a-failed-node" title="Permalink to this headline">¶</a></h2>
<p>If a node permanently fails scaling the deployment up and down may stop
working, or if a scaling operation is in progress it may get stuck
(because other nodes in the tier will wait forever for the failed node
to react). To recover from this situation the failed node should be
removed from the deployment using the following steps:</p>
<ul class="simple">
<li>Remove the node from the underlying etcd cluster. To do this:<ul>
<li>Run <code class="docutils literal"><span class="pre">clearwater-etcdctl</span> <span class="pre">cluster-health</span></code> and make a note of the
ID of the failed node.</li>
<li>Run <code class="docutils literal"><span class="pre">clearwater-etcdctl</span> <span class="pre">member</span> <span class="pre">list</span></code> to check that the failed
node reported is the one you were expecting (by looking at its IP
address).</li>
<li>Run <code class="docutils literal"><span class="pre">clearwater-etcdctl</span> <span class="pre">member</span> <span class="pre">remove</span> <span class="pre">&lt;ID&gt;</span></code>, replacing <code class="docutils literal"><span class="pre">&lt;ID&gt;</span></code>
with the ID learned above.</li>
</ul>
</li>
<li>Remove the failed node from any back-end data store clusters it was a
part of (see Removing a Node From a Data Store).</li>
</ul>
</div>
<div class="section" id="multiple-failed-nodes">
<h2>Multiple Failed Nodes<a class="headerlink" href="#multiple-failed-nodes" title="Permalink to this headline">¶</a></h2>
<p>If your deployment loses half or more of its nodes permanently, it loses
&#8220;quorum&#8221; which means that the underlying etcd cluster becomes read-only.
This means that scaling up and down is not possible and changes to
shared config cannot be made. It also means that the steps for removing
a single failed node won&#8217;t work. This section describes how to recover
from this state.</p>
<p>In this example, your initial cluster consists of servers A, B, C, D, E
and F. D, E and F die permanently, and A, B and C enter a read-only
state (because they lack quorum). Recent changes may have been
permanently lost at this point (if they were not replicated to the
surviving nodes).</p>
<p>You should follow this process completely - the behaviour is unspecified
if this process is started but not completed. It is always safe to
restart this process from the beginning (for example, if you encounter
an error partway through).</p>
<p>To recover from this state:</p>
<ul class="simple">
<li>stop etcd on A, B and C by running <code class="docutils literal"><span class="pre">sudo</span> <span class="pre">monit</span> <span class="pre">stop</span> <span class="pre">-g</span> <span class="pre">etcd</span></code></li>
<li>create a new cluster, only on A, by:<ul>
<li>editing <code class="docutils literal"><span class="pre">etcd_cluster</span></code> in <code class="docutils literal"><span class="pre">/etc/clearwater/local_config</span></code> to
just contain A&#8217;s IP (e.g. <code class="docutils literal"><span class="pre">etcd_cluster=10.0.0.1</span></code>)</li>
<li>running <code class="docutils literal"><span class="pre">sudo</span> <span class="pre">service</span> <span class="pre">clearwater-etcd</span> <span class="pre">force-new-cluster</span></code>. This
will warn that this is dangerous and should only be run during
this process; choose to proceed.</li>
<li>running <code class="docutils literal"><span class="pre">clearwater-etcdctl</span> <span class="pre">member</span> <span class="pre">list</span></code> to check that the
cluster only has A in</li>
<li>running <code class="docutils literal"><span class="pre">clearwater-etcdctl</span> <span class="pre">cluster-health</span></code> to check that the
cluster is healthy</li>
<li>running <code class="docutils literal"><span class="pre">clearwater-etcdctl</span> <span class="pre">get</span> <span class="pre">configuration/shared_config</span></code> to
check that the data is safe.</li>
<li>running <code class="docutils literal"><span class="pre">sudo</span> <span class="pre">monit</span> <span class="pre">monitor</span> <span class="pre">-g</span> <span class="pre">etcd</span></code> to put etcd back under
monit control</li>
</ul>
</li>
<li>get B to join that cluster by:<ul>
<li>editing <code class="docutils literal"><span class="pre">etcd_cluster</span></code> in <code class="docutils literal"><span class="pre">/etc/clearwater/local_config</span></code> to
just contain A&#8217;s IP (e.g. <code class="docutils literal"><span class="pre">etcd_cluster=10.0.0.1</span></code>)</li>
<li>running <code class="docutils literal"><span class="pre">sudo</span> <span class="pre">service</span> <span class="pre">clearwater-etcd</span> <span class="pre">force-decommission</span></code>. This
will warn that this is dangerous and offer the chance to cancel;
do not cancel.</li>
<li>running <code class="docutils literal"><span class="pre">sudo</span> <span class="pre">monit</span> <span class="pre">monitor</span> <span class="pre">-g</span> <span class="pre">etcd</span></code>.</li>
</ul>
</li>
<li>get C to join that cluster by following the same steps as for B:<ul>
<li>editing <code class="docutils literal"><span class="pre">etcd_cluster</span></code> in <code class="docutils literal"><span class="pre">/etc/clearwater/local_config</span></code> to
just contain A&#8217;s IP (e.g. <code class="docutils literal"><span class="pre">etcd_cluster=10.0.0.1</span></code>)</li>
<li>running <code class="docutils literal"><span class="pre">sudo</span> <span class="pre">service</span> <span class="pre">clearwater-etcd</span> <span class="pre">force-decommission</span></code>. This
will warn that this is dangerous and should only be run during
this process; choose to proceed.</li>
<li>running <code class="docutils literal"><span class="pre">sudo</span> <span class="pre">monit</span> <span class="pre">monitor</span> <span class="pre">-g</span> <span class="pre">etcd</span></code>.</li>
</ul>
</li>
<li>check that the cluster is now OK by doing the following on A:<ul>
<li>running <code class="docutils literal"><span class="pre">clearwater-etcdctl</span> <span class="pre">member</span> <span class="pre">list</span></code> to check that the
cluster now has A, B and C in</li>
<li>running <code class="docutils literal"><span class="pre">clearwater-etcdctl</span> <span class="pre">cluster-health</span></code> to check that the
cluster is healthy</li>
<li>running
<code class="docutils literal"><span class="pre">clearwater-etcdctl</span> <span class="pre">get</span> <span class="pre">clearwater/&lt;site_name&gt;/configuration/shared_config</span></code>
to check that the data is safe. The <code class="docutils literal"><span class="pre">site_name</span></code> is set in
<code class="docutils literal"><span class="pre">`local_config</span></code> &lt;<a class="reference external" href="http://clearwater.readthedocs.org/en/stable/Manual_Install/index.html#create-the-per-node-configuration">http://clearwater.readthedocs.org/en/stable/Manual_Install/index.html#create-the-per-node-configuration</a>&gt;`__
if the deployment is <a class="reference external" href="http://clearwater.readthedocs.org/en/stable/Geographic_redundancy/index.html">geographically
redundant</a>,
and defaults to <code class="docutils literal"><span class="pre">site1</span></code> if unset.</li>
</ul>
</li>
<li>log on to A. For each of D, E and F follow the instructions in
Removing a Node From a Data Store.</li>
</ul>
</div>
<div class="section" id="removing-a-node-from-a-data-store">
<h2>Removing a Node From a Data Store<a class="headerlink" href="#removing-a-node-from-a-data-store" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal"><span class="pre">mark_node_failed</span></code> script can be used to remove a failed node from
a back-end data store. You will need to know the type of the failed node
(e.g. &#8220;sprout&#8221;) and its IP address. To remove the failed node log onto a
working node in the same site and run the following commands (depending
on the failed node&#8217;s type). If you are removing multiple nodes
simultaneously, ensure that you run the <code class="docutils literal"><span class="pre">mark_node_failed</span></code> scripts for
each store type simultaneously (e.g. for multiple sprout removal, mark
all failed nodes for memcached simultaneously first, and then mark all
failed nodes for chronos).</p>
<p>If you cannot log into a working node in the same site (e.g. because an
entire geographically redundant site has been lost), you can use a
working node in the other site, but in this case you must run
<code class="docutils literal"><span class="pre">/usr/share/clearwater/clearwater-cluster-manager/scripts/mark_remote_node_failed</span></code>
instead of
<code class="docutils literal"><span class="pre">/usr/share/clearwater/clearwater-cluster-manager/scripts/mark_node_failed</span></code>.</p>
<p>If you are using separate signaling and management networks, you must
use the signaling IP address of the failed node as the failed node IP in
the commands below.</p>
<div class="section" id="sprout">
<h3>Sprout<a class="headerlink" href="#sprout" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo /usr/share/clearwater/clearwater-cluster-manager/scripts/mark_node_failed &quot;sprout&quot; &quot;memcached&quot; &lt;failed node IP&gt;
sudo /usr/share/clearwater/clearwater-cluster-manager/scripts/mark_node_failed &quot;sprout&quot; &quot;chronos&quot; &lt;failed node IP&gt;
</pre></div>
</div>
</div>
<div class="section" id="homestead">
<h3>Homestead<a class="headerlink" href="#homestead" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo /usr/share/clearwater/clearwater-cluster-manager/scripts/mark_node_failed &quot;homestead&quot; &quot;cassandra&quot; &lt;failed node IP&gt;
</pre></div>
</div>
</div>
<div class="section" id="homer">
<h3>Homer<a class="headerlink" href="#homer" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo /usr/share/clearwater/clearwater-cluster-manager/scripts/mark_node_failed &quot;homer&quot; &quot;cassandra&quot; &lt;failed node IP&gt;
</pre></div>
</div>
</div>
<div class="section" id="ralf">
<h3>Ralf<a class="headerlink" href="#ralf" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo /usr/share/clearwater/clearwater-cluster-manager/scripts/mark_node_failed &quot;ralf&quot; &quot;chronos&quot; &lt;failed node IP&gt;
sudo /usr/share/clearwater/clearwater-cluster-manager/scripts/mark_node_failed &quot;ralf&quot; &quot;memcached&quot; &lt;failed node IP&gt;
</pre></div>
</div>
</div>
<div class="section" id="memento">
<h3>Memento<a class="headerlink" href="#memento" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo /usr/share/clearwater/clearwater-cluster-manager/scripts/mark_node_failed &quot;memento&quot; &quot;cassandra&quot; &lt;failed node IP&gt;
sudo /usr/share/clearwater/clearwater-cluster-manager/scripts/mark_node_failed &quot;memento&quot; &quot;memcached&quot; &lt;failed node IP&gt;

## Complete Site Failure
</pre></div>
</div>
<p>In a geographically redundant deployment, you may encounter the
situation where an entire site has permanently failed (e.g. because the
location of that geographic site has been physically destroyed). To
recover from this situation:</p>
<ul class="simple">
<li>If the failed site contained half or more of your nodes, you have
lost quorum in your etcd cluster. You should follow the <a class="reference external" href="Handling_Failed_Nodes.md#multiple-failed-nodes">&#8220;Multiple
Failed Nodes&#8221;</a>
instructions above to rebuild the etcd cluster, containing only nodes
from the surviving site.</li>
<li>If the failed site contained fewer than half of your nodes, you have
not lost quorum in your etcd cluster. You should follow the
<a class="reference external" href="Handling_Failed_Nodes.md#removing-a-failed-node">&#8220;Removing a Failed
Node&#8221;</a>
instructions above to remove each failed node from the cluster.</li>
</ul>
<p>After following the above instructions, you will have removed the nodes
in the failed site from etcd, but not from the
Cassandra/Chronos/Memcached datastore clusters. To do this, follow the
<a class="reference external" href="Handling_Failed_Nodes.md#removing-a-node-from-a-data-store">&#8220;Removing a Node From a Data
Store&#8221;</a>
instructions above for each failed node, using the
<code class="docutils literal"><span class="pre">mark_remote_node_failed</span></code> script instead of the <code class="docutils literal"><span class="pre">mark_node_failed</span></code>
script.</p>
<p>You should now have a working single-site cluster, which can continue to
run as a single site, or be safely paired with a new remote site.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Dealing with Failed Nodes</a><ul>
<li><a class="reference internal" href="#removing-a-failed-node">Removing a Failed Node</a></li>
<li><a class="reference internal" href="#multiple-failed-nodes">Multiple Failed Nodes</a></li>
<li><a class="reference internal" href="#removing-a-node-from-a-data-store">Removing a Node From a Data Store</a><ul>
<li><a class="reference internal" href="#sprout">Sprout</a></li>
<li><a class="reference internal" href="#homestead">Homestead</a></li>
<li><a class="reference internal" href="#homer">Homer</a></li>
<li><a class="reference internal" href="#ralf">Ralf</a></li>
<li><a class="reference internal" href="#memento">Memento</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="Provisioning_Subscribers.html" title="previous chapter">Provisioning Subscribers</a></li>
      <li>Next: <a href="Clearwater_CPP_Coding_Guidelines.html" title="next chapter">C++ Coding Guidelines</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/Handling_Failed_Nodes.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Metaswitch Networks.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.3.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.7</a>
      
      |
      <a href="_sources/Handling_Failed_Nodes.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>